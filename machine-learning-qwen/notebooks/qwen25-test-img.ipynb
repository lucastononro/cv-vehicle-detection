{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3.11 install huggingface_hub --break-system-packages\n",
    "# !pip3.11 install git+https://github.com/huggingface/transformers accelerate --break-system-packages\n",
    "# !pip3.11 install qwen-vl-utils --break-system-packages\n",
    "# !pip3.11 install --pre torch==2.6.0 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "# !pip3 install -U flash-attn --no-build-isolation --break-system-packages #- not used for macbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# Set the device to MPS if available, else fallback to CPU\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "# Load model (using available device(s))\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=\"auto\", \n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\", # not avaialble for macbooks\n",
    ")\n",
    "\n",
    "# Move the model explicitly to the selected device\n",
    "model.to(device)\n",
    "\n",
    "# Load the processor for both text and vision modalities\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image 2 text inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image depicts a serene beach scene with a person and a dog. The person is sitting on the sandy beach, facing the ocean. They are wearing a plaid shirt and black pants, and they have long hair. The dog, which appears to be a Labrador Retriever, is sitting on the sand and is interacting with the person by placing its paw on their hand. The dog is wearing a harness with a colorful collar. The background shows the ocean with gentle waves and a clear sky, suggesting it might be early morning or late afternoon due to the soft lighting. The overall atmosphere of the image is peaceful and joyful.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define messages with an image and a text request\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare the input text using the processor's chat template\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Process the vision information (images and/or videos)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "# Prepare the full inputs for the model\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Move the inputs to the selected device\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Run inference (generate a response)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "# Trim the generated IDs to exclude the prompt part\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# Decode the generated tokens into text\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid THW: tensor([[ 1, 40, 60]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "def extract_image_embeddings(image_path, question=\"\", min_pixels=350000, max_pixels=500000):\n",
    "    # Create messages format similar to previous example\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image_path,\n",
    "                    \"max_pixels\": max_pixels,\n",
    "                    \"min_pixels\": min_pixels,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Process the vision information (images and/or videos)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    # Prepare the inputs\n",
    "    inputs = processor(\n",
    "        text=[\"\"],  # Empty text since we only need image embeddings\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the correct device\n",
    "    inputs = inputs.to(device)\n",
    "    pixel_values = inputs[\"pixel_values\"].type(torch.bfloat16)\n",
    "    \n",
    "    # Extract the visual embeddings using the visual component\n",
    "    with torch.no_grad():\n",
    "        # Access the visual component directly\n",
    "        vision_model = model.visual\n",
    "        image_embeds = vision_model(pixel_values, grid_thw=inputs[\"image_grid_thw\"])\n",
    "    \n",
    "    return inputs[\"image_grid_thw\"], image_embeds\n",
    "\n",
    "# Example usage\n",
    "image_path = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "grid_thw, img_embeddings = extract_image_embeddings(image_path)\n",
    "print(f\"Grid THW: {grid_thw}\")\n",
    "# print(f\"Embedding shape: {embeddings.last_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.9688, -2.0312,  1.2578,  ..., -2.0625,  0.9414,  0.1787],\n",
       "        [ 1.2656, -3.2656,  1.5156,  ..., -2.3906,  1.5469, -0.2188],\n",
       "        [ 1.2031,  0.0474,  1.0078,  ..., -1.4141,  1.6328,  0.2012],\n",
       "        ...,\n",
       "        [-1.4844, -1.2266, -0.5586,  ...,  1.3750, -0.2012, -0.9219],\n",
       "        [ 0.0425, -0.9922, -1.8984,  ...,  1.3203,  0.1289, -1.8516],\n",
       "        [ 0.3945, -4.0312,  0.1670,  ...,  0.5117,  1.3672, -1.5391]],\n",
       "       device='mps:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text embedding extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding shape: torch.Size([1, 125, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Get all the methods from model\n",
    "def extract_text_embeddings(text):\n",
    "    # Prepare the input text using the processor\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the correct device\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    # Extract the text embeddings using the decoder model\n",
    "    with torch.no_grad():\n",
    "        # Access the decoder model directly\n",
    "        decoder_model = model.model\n",
    "        outputs = decoder_model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        \n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"The image depicts a serene beach scene with a person and a dog. The person is sitting on the sandy beach, facing the ocean. They are wearing a plaid shirt and black pants, and they have long hair. The dog, which appears to be a Labrador Retriever, is sitting on the sand and is interacting with the person by placing its paw on their hand. The dog is wearing a harness with a colorful collar. The background shows the ocean with gentle waves and a clear sky, suggesting it might be early morning or late afternoon due to the soft lighting. The overall atmosphere of the image is peaceful and joyful\"\"\"\n",
    "\n",
    "text_embeddings = extract_text_embeddings(text)\n",
    "print(f\"Text embedding shape: {text_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4922, -1.7578, -2.3906,  ..., -1.1328,  9.9375, -0.1572],\n",
       "         [ 4.2188, -0.9023, -2.0938,  ..., -1.0938,  2.9219, -1.9062],\n",
       "         [-0.1592, -1.4531, -2.2031,  ..., -0.0221,  6.3438, -0.4980],\n",
       "         ...,\n",
       "         [ 0.4727, -4.0938,  2.3594,  ...,  1.3516,  7.1875, -3.1406],\n",
       "         [ 6.3125, -0.9336, -3.6562,  ..., -1.9219,  8.9375, -1.9297],\n",
       "         [-0.0109, -4.0000,  1.8984,  ..., -0.2676,  4.0938, -2.2344]]],\n",
       "       device='mps:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare text and image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 125, 2048])\n",
      "torch.Size([600, 2048])\n"
     ]
    }
   ],
   "source": [
    "# torch multiply both embeddings\n",
    "print(text_embeddings.shape)\n",
    "print(img_embeddings.shape)\n",
    "# # torch multiply both embeddings\n",
    "# text_embeddings * img_embeddings\n",
    "# # torch multiply both embeddings\n",
    "# text_embeddings * img_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.07763671875\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming text_embeddings has shape [1, 125, 2048] and img_embeddings has shape [600, 2048]\n",
    "\n",
    "# Aggregate text embeddings by mean pooling along the token dimension (dim=1)\n",
    "text_embedding = text_embeddings.mean(dim=1)  # Resulting shape: [1, 2048]\n",
    "\n",
    "# Aggregate image embeddings by mean pooling along the token dimension (dim=0)\n",
    "image_embedding = img_embeddings.mean(dim=0, keepdim=True)  # Resulting shape: [1, 2048]\n",
    "\n",
    "# Normalize both embeddings to unit vectors\n",
    "text_embedding = F.normalize(text_embedding, p=2, dim=-1)\n",
    "image_embedding = F.normalize(image_embedding, p=2, dim=-1)\n",
    "\n",
    "# Compute cosine similarity between the text and image embeddings\n",
    "cosine_similarity = (text_embedding * image_embedding).sum(dim=-1)\n",
    "print(\"Cosine similarity:\", cosine_similarity.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
